---
title: "Notes"
output: html_document
---

[prob]: http://www.sciweavers.org/tex2img.php?eq=%20%5Crho%20&bc=White&fc=Black&im=png&fs=12&ff=arev&edit=0

# Probability

![](http://upload.wikimedia.org/wikipedia/commons/f/fe/Stochastikmengen1.PNG)

If A and B are two independent events then the probability of them both occurring is the product of the probabilities.

$P(A \cap B) = P(A) * P(B)$

`P(A&B) = P(A) * P(B)`

If an event E can occur in more than one way and these ways are disjoint (mutually exclusive) then P(E) is the sum of the probabilities of each of the ways in which it can occur.

The probability of at least one of two events, A and B, occurring is the sum of their individual probabilities minus the probability of their intersection. 

$P(A \cup B) = P(A) + P(B) - P(A \cap B)$

`P(A U B) = P(A) + P(B) - P(A&B)`

It follows that if A and B are disjoint or mutually exclusive, i.e. only one of them can occur, then 

$P(A \cup B) = P(A)+P(B)$

`P(A U B) = P(A)+P(B)`

We represent the conditional probability of an event A given that B has occurred with the notation $P(A|B)$. More
specifically, we define the conditional probability of event A, given that B has occurred with the following.

`P(A|B) = P(A & B)/ P(B)` . $P(A|B)$ is the probability that BOTH A and B occur divided by the probability that B occurs.

$P(A|B) = P(A \cap B)/ P(B)$


This is a simple form of Bayes' Rule which relates the two conditional
probabilities.
$P(B|A) = P(B \cap A)/P(A) = P(A|B) * P(B)/P(A)$

P(B|A) = P(B&A)/P(A) = P(A|B) * P(B)/P(A)

Suppose we don't know P(A) itself, but only know its conditional probabilities, that is, the probability that it occurs if B occurs and the probability that it occurs if B doesn't occur. These are P(A|B) and P(A|~B), respectively. We use ~B to represent 'not B' or 'B complement'.

We can then express `P(A) = P(A|B) * P(B) + P(A|~B) * P(~B)` and substitute this is into the denominator of Bayes' Formula.


## Bayes' rule
Baye's rule allows us to reverse the conditioning set provided
that we know some marginal probabilities

$P(B ~|~ A) = \frac{P(A ~|~ B) P(B)}{P(A ~|~ B) P(B) + P(A ~|~ B^c)P(B^c)}$
  
`P(B|A) = P(A|B) * P(B) / ( P(A|B) * P(B) + P(A|~B) * P(~B) )`

### Diagnostic tests

- Let $+$ and $-$ be the events that the result of a diagnostic test is positive or negative respectively
- Let $D$ and $D^c$ be the event that the subject of the test has or does not have the disease respectively 
- The **sensitivity** is the probability that the test is positive given that the subject actually has the disease, $P(+ ~|~ D)$
- The **specificity** is the probability that the test is negative given that the subject does not have the disease, $P(- ~|~ D^c)$


### More definitions

- The **positive predictive value** is the probability that the subject has the  disease given that the test is positive, $P(D ~|~ +)$
- The **negative predictive value** is the probability that the subject does not have the disease given that the test is negative, $P(D^c ~|~ -)$
- The **prevalence of the disease** is the marginal probability of disease, $P(D)$


### More definitions

- The **diagnostic likelihood ratio of a positive test**, labeled $DLR_+$, is $P(+ ~|~ D) / P(+ ~|~ D^c)$, which is the $$sensitivity / (1 - specificity)$$
- The **diagnostic likelihood ratio of a negative test**, labeled $DLR_-$, is $P(- ~|~ D) / P(- ~|~ D^c)$, which is the $$(1 - sensitivity) / specificity$$

### Example

- A study comparing the efficacy of HIV tests, reports on an experiment which concluded that HIV antibody tests have a sensitivity of 99.7% and a specificity of 98.5%
- Suppose that a subject, from a population with a .1% prevalence of HIV, receives a positive test result. What is the positive predictive value?
- Mathematically, we want $P(D ~|~ +)$ given the sensitivity, $P(+ ~|~ D) = .997$, the specificity, $P(- ~|~ D^c) =.985$, and the prevalence $P(D) = .001$

### Using Bayes' formula

$$
\begin{eqnarray*}
  P(D ~|~ +) & = &\frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + P(+~|~D^c)P(D^c)}\\ \\
 & = & \frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + \{1-P(-~|~D^c)\}\{1 - P(D)\}} \\ \\
 & = & \frac{.997\times .001}{.997 \times .001 + .015 \times .999}\\ \\
 & = & .062
\end{eqnarray*}
$$

- In this population a positive test result only suggests a 6% probability that the subject has the disease 
- (The positive predictive value is 6% for this test)


## PMF - Probability mass function

A probability mass function (PMF) gives the probability that a discrete random variable is exactly equal to some value.

A probability mass function evaluated at a value corresponds to the probability that a random variable
take that value. To be a valid pmf a function $\rho$ must satify

- It must always be larger or equal to $0$
- The sum of the possible values that the random variable can take has to add up to one


## PDF - Probability density function

A probability density function is associated with a continuous random variable. To quote from Wikipedia, it "is a
function that describes the relative likelihood for this random variable to take on a given value. The probability of the random variable falling within a particular range of values is given by ... the area under the density function but above the horizontal axis and between the lowest and greatest values of the range."

A probability density function is a function associated with a continuous random variable. To be a valid pdf a function must satisfy

- It must be larger than or equal to zero everywhere
- The total are under it must be one

Areas under pdfs correspond to probabilities for that random variable

Example pdf: $f(x) = \begin{cases}2x & \mbox{ for }& 0 < x < 1\\0 & \mbox{ otherwise }\end{cases}$ 

```{r}
x <- c(-0.5, 0, 1, 1, 1.5)
y <- c(0, 0, 2, 0 ,0)
plot(x, y, lwd=3,type="l")
```

- always larger or equal to zero
- area is simply $length * height / 2$ of the triangle what is $1$

What is the probability that 75% or fewer of calls get addressed?

```{r}
plot(x, y, lwd = 3, type = "l")
polygon(c(0, .75, .75, 0), c(0, 0, 1.5, 0), lwd = 3, col = "lightblue")
```
```{r}
0.75 * 1.5 /2
pbeta(.75, 2, 1)
```

## CDF and survival function

The cumulative distribution function (CDF) of a random variable X, either discrete or continuous, is the function F(x)
equal to the probability that X is less than or equal to x. 

- The **cumulative distribution function** (CDF) of a random variable $X$ is defined as the function 
$$
F(x) = P(X \leq x)
$$
- This definition applies regardless of whether $X$ is discrete or continuous.
- The **survival function** of a random variable $X$ is defined as
$$
S(x) = P(X > x)
$$
- Notice that $S(x) = 1 - F(x)$
- For continuous random variables, the PDF is the derivative of the CDF

## Diagnostic Likelihood Ratio

The diagnostic likelihood ratio of a positive test, DLR_+, is the ratio of the two + 
conditional probabilities, one given the presence of disease and the other given the absence. Specifically, 
`DLR_+ = P(+|D) / P(+|~D)`. Similarly, the DLR_- is defined as a ratio

Recall that P(+|D) and P(-|~D), (test sensitivity and specificity respectively) are accuracy rates of a diagnostic test for the two possible results. They should be close to 1 because no one would take an inaccurate test, right? Since `DLR_+ = P(+|D) / P(+|~D)` we recognize the numerator as test sensitivity and the denominator as the complement of test specificity.

## Sensivity and Specificity

Suppose we know the accuracy rates of the test for both the positive case (positive result when the patient has HIV) and negative (negative test result when the patient doesn't have HIV). These are referred to as test sensitivity and
specificity, respectively.

## IID

Random variables are said to be iid if they are independent and identically distributed. By independent we mean "statistically unrelated from one another". Identically distributed means that "all have been drawn from the same population distribution"


## Expected Value

Variance: $E[x^2]-E[x]^2$

Mean of random variable: $E[X] = \sum_x x*p(x)$ `sum of:possible value * possibibility`

$E[X^2] = \sum x^2 * p(x)$

## Summary

A probability model connects data to a population using assumptions.

Be careful to distinguish between population medians and sample medians.

A sample median is an estimator of a population median (the estimand).


## Odds vs Probability

```
             (Chances for)
     P(x) = ---------------
            (Total chances)
            
            
     (Chances for) : (Chances against)
```

Draw an ace

```
p(x) = 4 / 52 = 0.07

odds d = 4:(52-4) = 4:48 = 1:12 


d = p / (1-p)

```