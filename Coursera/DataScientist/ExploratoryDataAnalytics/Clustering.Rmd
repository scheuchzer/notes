---
title: "Clustering"
output: html_document
---

Useful for visualizing highdimensional datasets.

Clustering organizes things that are close into groups.

- How do we define close?
- How do we group things?
- How do we visualize the grouping?
- How do we interpret the grouping?

http://rafalab.jhsph.edu/688/lec/lecture5-clustering.pdf

# How do we define close?

Most important step

- Garbage in -> Garbage out

Distance or similarity

- Contiuous - euclidean distance 
    - straight line between two points
- Contiuous - correlation similarity
- Binary - manhattan distance
    - http://en.wikipedia.org/wiki/Taxicab_geometry
    - ![](http://upload.wikimedia.org/wikipedia/commons/0/08/Manhattan_distance.svg)

Pick a distance/similiarity that makes sense for your problem.

# Merging

- complete linking
    - the center of the two points farest away in the cluster
- average linking
    - the center of the gravity center of all points in the cluster


# Hierarchical Clustering
  
An agglomerative approach:

- Find closest two things
- Put the together
- Find next closest
    
Requires:

- A defined distance
- A merging approach
    
Produces:

- A tree showing how close things are to each other

The picture may be unstable:

- Change a few points
- Have different missing values
- Pick a different distance
- Change the merging strategy
- Change the scale of points for one value

But it's deterministic.

Test data
```{r}
set.seed(1234)
par(mar = c(0,0,0,0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1,2,1), each = 4), sd = 0.2)
plot(x, y, col = "blue", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, labels = as.character(1:12))
```
calculate the eucledian distance

```{r}
data <- data.frame(x = x, y = y)
distxy <- dist(data)
```

Create a cluster dendogram
```{r}
hClustering <- hclust(distxy)
plot(hClustering)
```

## Heatmap

Runs a hierachical clustering on the rows of the dataset and a hierarchical clustering the columns as well.

```{r}
data <- data.frame(x = x, y = y)
set.seed(143)
dataMatrix <- as.matrix(data)[sample(1:12), ]
heatmap(dataMatrix)
````
# K-Means

A partitioning approach:

- Fix a number of clusters
- Get "centroids" of each cluster
- Assign things to closest centroid
- Recalculate centroids

Requires:

- A defined distance metric
- A number of clusters
- An initial guess as to cluster centroids

Produces:

- Final estimate of cluster centroid
- An assignment of each point to cluster

```{r}
data <- data.frame(x = x, y = y)
kmeansObj <- kmeans(data, centers = 3)
kmeansObj$cluster
par(mar = rep(0.2, 4))
plot(x, y, col = kmeansObj$cluster, pch = 19, cex = 2)
points(kmeansObj$centers, col = 1:3, pch = 3, cex = 3, lwd = 3)
```

# Dimension Reduction

```{r}
set.seed(12345)
dataMatrix <- matrix(rnorm(400), nrow = 40)
#image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])
set.seed(678910)
for (i in 1:40) {
    coinFlip <- rbinom(1, size = 1, prob = 0.5)
    if (coinFlip) {
        dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0,3), each = 5)
    }
}
#image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])
par(mar = rep(0.2, 4))
heatmap(dataMatrix)

hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order, ]
par(mar = c(4,4,4,4))
par(mfrow = c(1, 3))
image(1:10, 1:40, t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rowMeans(dataMatrixOrdered), 40:1, , xlab = "Row Mean", ylab = "Row", pch = 19)
plot(colMeans(dataMatrixOrdered), xlab = "Column", ylab = "Column Mean", pch = 19)
```

# Principal Components Analysis and Singular Value Decomposition

## Single Value Decomposision (SVD)

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(svd1$u[,1], 40:1, , xlab = "Row", ylab = "First left singular vector", pch = 19)
plot(svd1$v[,1], xlab = "Column", ylab = "First right singular vector", pch = 19)
```

We now found a pattern similar to the pattern in the previous example but without knowing that we should look
at the means of the rows or columns.

### Variance explained

```{r}
par(mfrow = c(1,2))
plot(svd1$d, xlab = "Column", ylab = "Singlular value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Prob. of variance explained", pch = 19)
```

Almost half (40%) of the data is exlained with a single variable.

Simple example

```{r}
constantMatrix <- matrix(nrow = 40, ncol = 10)
# Fill matrix with 5 x 0 and 5 x 1 for each row
for (i in 1:dim(constantMatrix)[1]){ constantMatrix[i, ] <- rep(c(0,1), each=5)}
svd1 <- svd(constantMatrix)
par(mfrow = c(1,3))
image(t(constantMatrix)[, nrow(constantMatrix):1])
plot(svd1$d, xlab="Column", ylab="Singluar value", pch=19)
plot(svd1$d^2/sum(svd1$d^2), xlab="Column", ylab="Prob. of variance explained", pch = 19)
```

Even tough we have lots of variables everything is explained with one single component.

Add a second pattern

```{r}
set.seed(678910)
for (i in 1:40) {
    coinFlip1 <- rbinom(1, size=1, prob=0.5)
    coinFlip2 <- rbinom(1, size=1, prob=0.5)
    # if coin is head add a common pattern to that row
    if (coinFlip1) {
        dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0,5), each=5)
    }
    if (coinFlip2) {
        dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0,5), 5)
    }
}
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order,]
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1,3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
# This is the truth (that we later try to find without known before)
# Pattern 1: first five columnd have a mean of 0, then 5 columns 1
plot(rep(c(0,1), each=5), pch=19, xlab="Column", ylab="Pattern 1")
# Pattern 2: Alternating mean of 0 and 1 for column 1 to 10
plot(rep(c(0,1), 5), pch=19, xlab="Column", ylab="Pattern 2")

image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
# What patterns does SVD find?
# Pattern 1: there's a difference between the first 5 columns and the last 5 column 
plot(svd2$v[,1], pch=19, xlab="Column", ylab="Pattern 1")
# Pattern 2: ther's an alternating visible but not as clear as in the truth plot, but it's there (but not obious)
plot(svd2$v[,2], pch=19, xlab="Column", ylab="Pattern 2")
```

variance explained

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1,2))
plot(svd1$d, xlab="Column", ylab="Singluar value", pch=19)
plot(svd1$d^2/sum(svd1$d^2), xlab="Column", ylab="Prob. of variance explained", pch=19)
```
First component explains 50% of the data. This is because the shift pattern (0,0,0,0,0,1,1,1,1,1) is very strong. The alternating pattern is much harder to pick up.



## Relationship to principal components

Actually SVD and PCA produce a similar result.

```{r}
par(mfrow = c(1,1))
svd1 <- svd(scale(dataMatrixOrdered))
pca1 <- prcomp(dataMatrixOrdered, scale = TRUE)
plot(pca1$rotation[,1], svd1$v[,1], pch = 19, xlab = "Principal Component 1", ylab = "Right Singular Vector 1")
abline(c(0,1))
```

## Principal Compmonents Analysis (PCA)



## Face example

```{r}
library (png)
library(caTools)
library(tiff)
img <- readTIFF("tho.tiff")
rasterImage(img, 1.2, 1.27, 1.8, 1.73, interpolate=TRUE)
transparent <- img[,,4] == 0
img <- as.raster(img[,,1:3])
img[transparent] <- NA
image(img)
```


# impute Package - Missing values

Library for filling NAs with reasonable values.

`impute.knn()`: impute missing expression data, using nearest neighbour averaging

```r
source("http://bioconductor.org/biocLite.R")
biocLite("impute")
```

```{r}
library(impute)
dataMatrix2 <- dataMatrixOrdered
# Add some missing values
dataMatrix2[sample(1:100, size=40, replace=FALSE)] <- NA
# Fix the missing values with nearest neighour averaging
dataMatrix2 <- impute.knn(dataMatrix2)$data
svd1 <- svd(scale(dataMatrixOrdered))
svd2 <- svd(scale(dataMatrix2))
# compare the results of the original matrix and the fixed matrix
par(mfrow = c(1,2))
plot(svd1$v[,1], pch=19)
plot(svd2$v[,1], pch=19)
```
